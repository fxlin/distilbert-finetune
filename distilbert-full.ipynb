{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc2a082-418b-4252-ac77-4fd04360ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3 \n",
    "\n",
    "#!pip install datasets transformers[sentencepiece]\n",
    "#!pip install ipywidgets\n",
    "#!pip install torchsummary \n",
    "#!pip install accelerate \n",
    "\n",
    "# cf: Huggingface\n",
    "# https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb#scrollTo=nZ09SFwfjXNO\n",
    "# Google\n",
    "# https://colab.research.google.com/github/501Good/tartu-nlp-2020/blob/master/labs/lab6/Lab6_TransformersClassification.ipynb#scrollTo=yfUd-Wr3t7rO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812ab5e7-0a11-4244-8d32-e7351114ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72bb884-d550-4348-9ffb-96a9139905fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pre-trained\n",
    "# #  ... many weights are not init'd... random??\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "# metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068c3483-8562-4e7e-8134-96c5779a5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dump model \n",
    "#\n",
    "# cf: \n",
    "# https://stackoverflow.com/questions/68577198/pytorch-summary-fails-with-huggingface-model\n",
    "# https://stackoverflow.com/questions/68585678/pytorch-summary-fails-with-huggingface-model-ii-expected-all-tensors-to-be-on-t\n",
    "# torch info doc: \n",
    "# https://github.com/TylerYep/torchinfo\n",
    "# ... works, but not very helpful\n",
    "# summary(model,input_size=(8,512), dtypes=['torch.IntTensor'], device='cpu',verbose=1) \n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    if param.requires_grad:\n",
    "#        print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd25d66e-80eb-4d60-94ac-0c44af16e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # save model ...\n",
    "# ########################\n",
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './model_save/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7dbb89-e0e3-48bf-9b2c-c3cd55a37bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "metric = load_metric(\"accuracy\")\n",
    "# Copy the model to the GPU.\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# freeze transformer layers\n",
    "# for param in model.distilbert.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e72317-77d6-4415-8090-f57c568d44e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xzl: this???\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def to_bin_class(ex):\n",
    "\tex['label'] = round(ex['label'])\n",
    "\treturn ex\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "\treturn tokenizer(ex['sentence'], padding='max_length', truncation=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\tlogits, labels = eval_pred\n",
    "\tpreds = np.argmax(logits, axis=-1)\n",
    "\treturn metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e266f4-7333-4061-98d8-e4a0d793b1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sst (/u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1121a0f234844f8380efb627f746b38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-175e01bd44ae6e77.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-bcfe3eb7a84204f3.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-d17996794240dda4.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-a4488ee52a49920a.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-1c1de7b235a26b22.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-e13f66f58e1dc770.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-3f63b4244a331410.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-6340d14986ce5b0f.arrow\n",
      "Loading cached processed dataset at /u/xl6yq/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-8503d683ae182269.arrow\n"
     ]
    }
   ],
   "source": [
    "sst = load_dataset('sst', 'default')\n",
    "sst = sst.remove_columns(['tokens', 'tree'])\n",
    "sst = sst.map(to_bin_class)\n",
    "sst = sst.cast_column('label', ClassLabel(num_classes=2))\n",
    "\n",
    "sst_tokenized = sst.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941fb3be-7aea-47ab-83b3-634e1247593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "sst_tokenized = sst_tokenized.remove_columns(['sentence'])\n",
    "\n",
    "trn_set = sst_tokenized['train']\n",
    "num_trn = trn_set.num_rows\n",
    "trn_set_10 = torch.utils.data.Subset(trn_set, list(range(0,int(num_trn/10))))\n",
    "trn_set_50 = torch.utils.data.Subset(trn_set, list(range(0,int(num_trn/2))))\n",
    "tst_set = sst_tokenized['test']\n",
    "val_set = sst_tokenized['validation']\n",
    "\n",
    "print(val_set.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51370166-f49a-4af6-a17d-5e5f34e8017e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loader, split into train/eval sets\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "   trn_set, shuffle=True, batch_size=8, collate_fn=data_collator\n",
    "   # trn_set_100, shuffle=False, batch_size=8, collate_fn=data_collator\n",
    "    # trn_set_500, shuffle=False, batch_size=8, collate_fn=data_collator    \n",
    "    # trn_set_1000, shuffle=False, batch_size=8, collate_fn=data_collator    \n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    val_set, batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db319f2-fadd-4e4f-afec-09f04a431300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1036,  5650,  ...,     0,     0,     0],\n",
      "        [  101,  2712,  9692,  ...,     0,     0,     0],\n",
      "        [  101,  8494, 20043,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  4107,  1037,  ...,     0,     0,     0],\n",
      "        [  101,  2009,  2003,  ...,     0,     0,     0],\n",
      "        [  101,  2045,  2003,  ...,     0,     0,     0]])\n",
      "train #batches= 1068\n",
      "val #batches= 138\n"
     ]
    }
   ],
   "source": [
    "# sanity check data ...\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}    \n",
    "#batch.items()\n",
    "print(batch['input_ids'])\n",
    "print(\"train #batches=\",len(train_dataloader))\n",
    "print(\"val #batches=\",len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9902d696-e69b-4d32-94b7-8b6339f35a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7152, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n",
      "SequenceClassifierOutput(loss=tensor(0.7152, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0666,  0.0467],\n",
      "        [-0.1223,  0.0445],\n",
      "        [-0.0278,  0.0689],\n",
      "        [-0.0510,  0.0634],\n",
      "        [-0.0706,  0.0964],\n",
      "        [-0.0897,  0.0572],\n",
      "        [-0.0698,  0.0023],\n",
      "        [-0.0405,  0.0681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[0.4717, 0.5283],\n",
      "        [0.4584, 0.5416],\n",
      "        [0.4758, 0.5242],\n",
      "        [0.4714, 0.5286],\n",
      "        [0.4584, 0.5416],\n",
      "        [0.4634, 0.5366],\n",
      "        [0.4820, 0.5180],\n",
      "        [0.4729, 0.5271]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# run one batch\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "print(outputs)\n",
    "# probablity\n",
    "print(torch.nn.functional.softmax((outputs.logits),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58a1b57-066d-426b-9ab8-a9af38583e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor t in optimizer.param_groups[0]['params']:\\n    print(t.shape)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... prep optim lr scheduler etc  ... \n",
    "#from transformers import AdamW    # deprecated\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# optimizer.param_groups[0]['params']\n",
    "# print(len(optimizer.param_groups[0]['params']))\n",
    "\n",
    "'''\n",
    "for t in optimizer.param_groups[0]['params']:\n",
    "    print(t.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11fc988-40fa-4023-935f-15dffa473546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3204\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3    # xzl. default:3\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "# print(model)  # can be useful\n",
    "\n",
    "#summary(model,input_size=(8,512)) # ... type errors??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d540a53-4987-4307-bfd1-29f763cf0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# excluded_batches = []\n",
    "# bottom 50%\n",
    "# excluded_batches = [0,2,6,5,1,4,3,8,7,10,9,11,581,870,525,785,12,13,827,811,422,973,1021,14,320,967,140,122,583,624,935,542,936,924,618,777,22,591,888,331,32,593,852,813,297,825,15,156,687,30,75,1029,643,538,774,19,530,234,287,25,724,943,316,93,978,427,384,611,464,750,900,361,432,519,208,760,553,625,336,982,111,179,312,104,471,26,845,17,68,855,695,139,293,177,552,144,897,548,706,1036,856,379,482,902,627,500,817,814,415,601,18,726,557,20,860,100,229,249,332,103,355,948,939,1049,824,579,822,895,450,1027,896,467,80,456,266,202,736,887,260,185,1062,974,1028,1060,781,933,165,309,255,480,240,700,108,232,461,993,184,381,458,296,1022,88,1001,395,323,91,1000,69,991,330,359,965,727,63,851,347,253,995,916,697,917,584,152,690,616,957,463,1006,843,76,466,378,135,663,286,50,770,410,539,201,92,879,506,647,70,236,245,998,181,248,404,741,29,954,209,306,246,119,268,949,133,1026,1056,418,449,196,682,221,918,863,806,1019,992,65,125,24,821,66,692,56,739,243,617,554,1015,397,550,416,79,748,95,406,409,149,1017,89,281,174,512,367,469,445,696,251,648,753,944,121,694,702,54,592,575,675,926,990,264,284,205,150,123,963,167,180,898,613,755,794,16,118,448,36,884,176,681,674,454,913,72,844,84,128,776,485,130,861,452,385,164,772,529,834,319,838,789,227,883,1011,468,988,479,672,274,803,407,1058,722,265,77,701,254,258,388,346,622,31,460,572,1033,413,698,484,38,612,1013,805,473,890,962,751,573,1047,637,462,389,956,113,951,927,375,937,282,82,197,244,971,168,98,33,964,368,382,451,1010,64,55,163,365,1031,537,1039,829,328,1025,603,833,641,324,711,880,486,459,1065,629,650,470,373,420,457,653,220,472,147,666,83,533,223,496,656,160,439,1023,947,1018,742,294,713,508,878,858,301,910,735,975,871,498,795,175,731,200,109,350,693,628,1024,28,157,342,730,551,646,1012,351,516,819,279,638,433,901,532,831,792,396,689,455,985,21,931,363,968,972,57,137,941,411,453,699,154,477,414,986,345,81,492,143,73,679,474,1009,206,425,952,145,1067,210,124,981,364,830,652,290,169,872,676,358,798,1005,899,773,906,636,678,325,869,172,400,252,846,441,745,570,1037,510,131,1064,737,606,644,231,360,1061,215,534,170,178,857,1032,408,994,812,1007,651,784,356,]\n",
    "# bottom 75%\n",
    "# excluded_batches = [16,4,12,18,6,7,22,13,9,19,11,5,2,14,17,20,25,21,15,3,10,27,24,8,1,29,23,30,28,26,0,32,31,670,34,630,943,168,979,225,33,92,940,599,595,989,456,1064,930,131,106,432,35,695,916,726,592,1014,453,729,565,622,117,806,637,258,1061,36,672,783,846,750,908,794,490,312,378,484,1047,814,144,980,854,947,145,404,318,566,781,498,506,1013,971,874,159,613,83,692,681,689,118,699,259,289,183,77,1002,128,60,415,417,160,1018,151,130,910,583,1043,679,684,139,1017,43,1010,757,1006,765,52,157,995,928,277,460,402,718,913,500,147,122,898,996,61,970,288,135,133,124,962,522,994,655,40,441,37,81,576,349,245,138,134,467,1031,38,50,529,774,961,333,914,45,915,963,905,74,152,964,430,362,330,201,331,395,337,1001,79,984,525,357,303,323,901,374,297,80,585,638,226,548,344,615,266,835,591,517,62,938,355,360,519,274,140,602,890,184,353,804,667,559,717,701,586,683,429,46,265,189,568,69,324,364,923,1036,499,472,937,827,521,82,85,343,687,282,438,792,745,222,72,112,580,171,464,927,291,121,120,268,569,75,925,627,501,748,546,55,785,1027,166,639,992,893,494,551,71,238,485,713,818,39,444,839,41,359,326,363,563,70,1011,953,823,990,673,325,859,78,746,843,656,960,669,577,1042,316,101,919,273,252,126,396,153,346,508,883,663,177,575,668,845,798,696,856,84,716,286,997,132,405,371,777,561,345,180,398,653,311,877,579,719,214,423,918,67,205,53,113,116,832,369,208,191,966,791,452,768,356,632,676,751,269,448,424,541,123,816,1055,550,873,865,380,372,657,926,543,704,86,851,1040,457,641,974,564,137,281,942,694,707,150,219,270,256,413,658,470,891,427,1065,944,648,742,428,262,852,284,903,597,520,414,193,198,838,691,301,626,549,959,710,909,253,954,310,581,574,382,786,491,711,473,685,836,931,620,154,584,329,878,65,876,848,59,475,822,706,42,820,682,93,828,801,367,322,451,882,635,279,594,532,582,573,217,264,216,867,866,129,978,1041,693,304,870,934,476,437,879,283,255,612,1054,624,233,513,739,425,643,320,1046,795,531,54,88,110,342,347,759,702,540,784,552,468,678,336,148,948,474,1039,246,642,976,399,1050,528,410,351,335,842,275,250,847,922,523,1032,1025,956,115,659,1058,334,76,190,267,108,778,315,932,125,486,888,352,516,988,633,397,578,478,221,1052,800,524,629,570,744,287,588,951,872,408,730,477,981,993,158,892,858,375,810,299,769,447,459,697,767,298,728,731,849,857,841,601,686,544,790,675,203,1004,1016,366,542,328,714,646,920,703,507,143,605,621,212,1020,412,753,977,911,936,808,454,889,338,982,518,896,560,272,443,276,509,57,824,895,593,502,1044,493,327,952,1051,875,625,688,645,63,793,787,73,619,1021,611,881,211,512,321,674,392,618,897,871,278,271,945,819,975,377,571,407,481,1045,662,724,169,426,968,339,572,194,370,439,230,514,261,463,983,383,368,782,607,972,213,306,254,234,690,812,533,1059,434,955,887,526,244,156,406,985,912,465,48,864,141,341,361,698,302,290,803,249,965,598,294,207,829,805,869,700,431,422,680,204,305,884,178,44,241,416,1009,243,192,537,248,202,98,837,538,365,215,536,181,1037,285,752,487,868,661,715,1003,175,182,530,164,247,419,185,949,223,172,732,562,723,503,917,1057,142,236,280,296,587,721,821,505,545,608,1030,780,735,87,206,969,497,558,567,504,96,308,257,89,99,510,1053,97,95,834,797,736,483,492,855,939,830,802,924,666,471,705,167,603,853,]\n",
    "\n",
    "# -- include all --- #\n",
    "\n",
    "included_batches = list(range(0, len(train_dataloader)))\n",
    "\n",
    "# loss, top 10 \n",
    "# included_batches = [0,12,606,28,908,14,629,647,229,961,683,156,921,84,868,74,1031,801,7,296,948,219,525,343,624,29,466,56,203,55,436,260,643,266,11,153,112,636,308,664,781,927,106,1024,465,197,726,19,278,90,173,50,533,53,42,925,357,77,902,612,281,318,799,43,911,263,422,111,259,198,71,225,482,832,723,782,790,588,476,689,904,383,479,631,139,556,771,1061,253,51,719,728,941,392,344,395,783,456,538,655,973,968,282,83,965,756,]\n",
    "# bottom 10\n",
    "# included_batches = [593,1029,414,1009,574,529,417,678,966,398,843,616,594,869,597,651,998,1059,1051,338,402,899,875,931,883,129,1034,166,496,422,1013,695,781,412,485,1044,440,708,240,792,773,935,785,816,905,689,624,321,364,249,383,775,863,854,937,757,667,587,558,945,310,745,445,435,599,648,968,735,600,965,1060,893,924,1043,231,81,888,592,608,1019,404,256,575,856,679,956,581,641,343,583,643,507,582,699,761,842,436,397,1052,1066,248,330,782,783,395,391,]\n",
    "\n",
    "skip_counter = 0\n",
    "loss_history = []\n",
    "loss_thershold = 2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165d98de-5345-4cc6-81c1-19471a49d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,068.    Elapsed: 0m 10s.\n",
      "  Batch    80  of  1,068.    Elapsed: 0m 21s.\n",
      "  Batch   120  of  1,068.    Elapsed: 0m 32s.\n",
      "  Batch   160  of  1,068.    Elapsed: 0m 43s.\n",
      "  Batch   200  of  1,068.    Elapsed: 0m 54s.\n",
      "  Batch   240  of  1,068.    Elapsed: 1m 5s.\n",
      "  Batch   280  of  1,068.    Elapsed: 1m 17s.\n",
      "  Batch   320  of  1,068.    Elapsed: 1m 28s.\n",
      "  Batch   360  of  1,068.    Elapsed: 1m 39s.\n",
      "  Batch   400  of  1,068.    Elapsed: 1m 51s.\n",
      "  Batch   440  of  1,068.    Elapsed: 2m 2s.\n",
      "  Batch   480  of  1,068.    Elapsed: 2m 13s.\n",
      "  Batch   520  of  1,068.    Elapsed: 2m 25s.\n",
      "  Batch   560  of  1,068.    Elapsed: 2m 36s.\n",
      "  Batch   600  of  1,068.    Elapsed: 2m 48s.\n",
      "  Batch   640  of  1,068.    Elapsed: 2m 59s.\n",
      "  Batch   680  of  1,068.    Elapsed: 3m 11s.\n",
      "  Batch   720  of  1,068.    Elapsed: 3m 22s.\n",
      "  Batch   760  of  1,068.    Elapsed: 3m 34s.\n",
      "  Batch   800  of  1,068.    Elapsed: 3m 45s.\n",
      "  Batch   840  of  1,068.    Elapsed: 3m 56s.\n",
      "  Batch   880  of  1,068.    Elapsed: 4m 8s.\n",
      "  Batch   920  of  1,068.    Elapsed: 4m 19s.\n",
      "  Batch   960  of  1,068.    Elapsed: 4m 31s.\n",
      "  Batch 1,000  of  1,068.    Elapsed: 4m 42s.\n",
      "  Batch 1,040  of  1,068.    Elapsed: 4m 54s.\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epcoh took: 5m 1s\n",
      "  skip_counter 0 0.00\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0m 13s\n",
      "Accuracy:  {'accuracy': 0.818346957311535}\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,068.    Elapsed: 0m 11s.\n",
      "  Batch    80  of  1,068.    Elapsed: 0m 22s.\n",
      "  Batch   120  of  1,068.    Elapsed: 0m 33s.\n",
      "  Batch   160  of  1,068.    Elapsed: 0m 45s.\n",
      "  Batch   200  of  1,068.    Elapsed: 0m 56s.\n",
      "  Batch   240  of  1,068.    Elapsed: 1m 8s.\n",
      "  Batch   280  of  1,068.    Elapsed: 1m 19s.\n",
      "  Batch   320  of  1,068.    Elapsed: 1m 31s.\n",
      "  Batch   360  of  1,068.    Elapsed: 1m 42s.\n",
      "  Batch   400  of  1,068.    Elapsed: 1m 54s.\n",
      "  Batch   440  of  1,068.    Elapsed: 2m 5s.\n",
      "  Batch   480  of  1,068.    Elapsed: 2m 16s.\n",
      "  Batch   520  of  1,068.    Elapsed: 2m 28s.\n",
      "  Batch   560  of  1,068.    Elapsed: 2m 39s.\n",
      "  Batch   600  of  1,068.    Elapsed: 2m 51s.\n",
      "  Batch   640  of  1,068.    Elapsed: 3m 2s.\n",
      "  Batch   680  of  1,068.    Elapsed: 3m 14s.\n",
      "  Batch   720  of  1,068.    Elapsed: 3m 25s.\n",
      "  Batch   760  of  1,068.    Elapsed: 3m 37s.\n",
      "  Batch   800  of  1,068.    Elapsed: 3m 48s.\n",
      "  Batch   840  of  1,068.    Elapsed: 3m 59s.\n",
      "  Batch   880  of  1,068.    Elapsed: 4m 11s.\n",
      "  Batch   920  of  1,068.    Elapsed: 4m 22s.\n",
      "  Batch   960  of  1,068.    Elapsed: 4m 33s.\n",
      "  Batch 1,000  of  1,068.    Elapsed: 4m 44s.\n",
      "  Batch 1,040  of  1,068.    Elapsed: 4m 56s.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 5m 4s\n",
      "  skip_counter 0 0.00\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0m 13s\n",
      "Accuracy:  {'accuracy': 0.8401453224341507}\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,068.    Elapsed: 0m 11s.\n",
      "  Batch    80  of  1,068.    Elapsed: 0m 22s.\n",
      "  Batch   120  of  1,068.    Elapsed: 0m 34s.\n",
      "  Batch   160  of  1,068.    Elapsed: 0m 45s.\n",
      "  Batch   200  of  1,068.    Elapsed: 0m 57s.\n",
      "  Batch   240  of  1,068.    Elapsed: 1m 8s.\n",
      "  Batch   280  of  1,068.    Elapsed: 1m 20s.\n",
      "  Batch   320  of  1,068.    Elapsed: 1m 31s.\n",
      "  Batch   360  of  1,068.    Elapsed: 1m 42s.\n",
      "  Batch   400  of  1,068.    Elapsed: 1m 54s.\n",
      "  Batch   440  of  1,068.    Elapsed: 2m 5s.\n",
      "  Batch   480  of  1,068.    Elapsed: 2m 17s.\n",
      "  Batch   520  of  1,068.    Elapsed: 2m 28s.\n",
      "  Batch   560  of  1,068.    Elapsed: 2m 40s.\n",
      "  Batch   600  of  1,068.    Elapsed: 2m 51s.\n",
      "  Batch   640  of  1,068.    Elapsed: 3m 2s.\n",
      "  Batch   680  of  1,068.    Elapsed: 3m 14s.\n",
      "  Batch   720  of  1,068.    Elapsed: 3m 25s.\n",
      "  Batch   760  of  1,068.    Elapsed: 3m 36s.\n",
      "  Batch   800  of  1,068.    Elapsed: 3m 48s.\n",
      "  Batch   840  of  1,068.    Elapsed: 3m 59s.\n",
      "  Batch   880  of  1,068.    Elapsed: 4m 10s.\n",
      "  Batch   920  of  1,068.    Elapsed: 4m 22s.\n",
      "  Batch   960  of  1,068.    Elapsed: 4m 33s.\n",
      "  Batch 1,000  of  1,068.    Elapsed: 4m 45s.\n",
      "  Batch 1,040  of  1,068.    Elapsed: 4m 56s.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 5m 4s\n",
      "  skip_counter 0 0.00\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0m 13s\n",
      "Accuracy:  {'accuracy': 0.8383287920072662}\n"
     ]
    }
   ],
   "source": [
    "# train/validation per epoch\n",
    "loss_values = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if not step in included_batches:\n",
    "            #print(\"skip batch\", step)\n",
    "            skip_counter += 1\n",
    "            continue\n",
    "                        \n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed_mins, elapsed_secs = epoch_time(t0, time.time())            \n",
    "            # Report progress.\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed_mins:}m {elapsed_secs:}s.')\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # loss is a tensor containing a single val\n",
    "        total_loss += loss.item()\n",
    "        #print(\"xzl:loss\", loss.item())\n",
    "        loss_history.append((step, loss.item()))\n",
    "                \n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)        \n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)     \n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(\"  Training epcoh took: {:}m {:}s\".format(*epoch_time(t0, time.time())))    \n",
    "    print(\"  skip_counter\", skip_counter, f\"{skip_counter/len(train_dataloader):.2f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    print(\"  Validation took: {:}m {:}s\".format(*epoch_time(t0, time.time())))\n",
    "    print(\"Accuracy: \", metric.compute())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec42ea7c-1556-45e1-ad6f-14690989bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041,915,837,843,699,898,968,758,1005,844,712,890,743,1015,1045,798,852,708,1052,635,212,483,965,703,716,829,936,523,1049,175,689,215,1044,217,484,209,881,370,490,985,597,991,230,197,861,180,393,585,425,152,136,934,641,776,342,118,507,1053,1065,222,953,932,870,510,153,186,301,831,571,438,350,643,1058,714,549,697,952,285,567,586,300,960,711,199,241,264,651,589,654,315,325,789,814,1034,611,757,501,969,992,780,176,270,642,358,1022,645,794,801,276,184,620,872,863,391,164,566,163,941,458,816,768,548,873,384,479,267,541,875,820,886,940,392,326,803,945,765,255,624,821,556,740,178,471,1017,950,627,259,478,356,251,998,784,772,907,949,917,195,353,759,383,653,717,119,421,621,927,974,749,1001,288,323,365,1055,609,1028,262,111,797,1054,788,885,688,244,866,996,913,246,761,231,404,378,256,1031,628,537,506,337,520,306,329,1029,150,849,855,228,633,550,551,824,555,309,327,311,783,448,221,92,472,644,607,661,291,456,182,755,405,187,496,210,769,154,134,371,622,145,610,632,997,140,360,971,441,333,646,374,818,909,977,399,524,289,897,261,517,1066,554,85,580,185,865,983,726,1014,947,1020,921,107,853,763,278,762,341,677,540,869,243,946,452,981,841,334,787,618,606,955,190,167,874,366,979,508,704,372,547,1040,55,1038,672,437,379,756,536,1043,294,924,930,916,247,682,675,819,273,77,989,232,363,925,713,63,110,"
     ]
    }
   ],
   "source": [
    "#print(optimizer.state['grad_history'])\n",
    "\n",
    "# gradient ... low to high\n",
    "# s = sorted(optimizer.state['grad_history'], key = lambda x: x[1])    \n",
    "# loss  ... low to high\n",
    "s = sorted(loss_history, key = lambda x: x[1])    \n",
    "\n",
    "#print(s)\n",
    "#print(s[int(9*len(s)/10):-1])  # top 10% \n",
    "#print(s[90:-1])\n",
    "bottom10 = s[0:int(1*len(s)/10)]\n",
    "bottom50 = s[0:int(5*len(s)/10)]\n",
    "bottom75 = s[0:int(7.5*len(s)/10)]\n",
    "bottom90 = s[0:int(9*len(s)/10)]\n",
    "\n",
    "top10 = s[int(9*len(s)/10):-1]\n",
    "top25 = s[int(7.5*len(s)/10):-1]\n",
    "\n",
    "#print(\"bot50\", bottom50)\n",
    "#print(\"bot50\", bottom50[-1])\n",
    "#print(\"top10\", top10)\n",
    "#print(\"top10\", top10[0])\n",
    "\n",
    "included_batches = [x[0] for x in bottom10] \n",
    "for x in included_batches:\n",
    "    print(x, end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5444609-804f-4297-92a6-0704245d8bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdc76d5047b4e74b8b1274787977648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train all epoches (no validation)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384d11c-028d-45c9-bd49-c0a259a3818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
